{\rtf1\ansi\ansicpg1252\deff0\nouicompat{\fonttbl{\f0\fnil\fcharset0 Courier New;}{\f1\fnil\fcharset238 Courier New;}{\f2\fnil\fcharset2 Symbol;}}
{\*\generator Riched20 10.0.19041}\viewkind4\uc1 
\pard\f0\fs22\lang1033 1. Decision Tree\par
\par
\f1\lang1045 My decision tree is stored as a class. I also use two helper classes: Node, and Leaf for storing nodes of the decision tree. The Decision Tree class consists of the root - first node of the tree, max_depth - maximum height of the tree, and feature_dim - the dimension of the sample data. The Node class stores the feature index that the split is done on, and right and left subtree. The Leaf class contains the value of the leaf (i.e. 0.0 - no, and 1.0 - yes) and accuracy of the leaf.\line Decision tree consists of multiple helper functions described here:\par

\pard{\pntext\f2\'B7\tab}{\*\pn\pnlvlblt\pnf2\pnindent0{\pntxtb\'B7}}\fi-360\li720 entropy(labels) - returns entropy of the set of labels. If the number of elements is less tha or equal to 1 the entropy is 0. else, the counts of each class are calculated using numpy unique function and then probabilities of each class is stored in the list. Then those probabilities are used to calculate the entropy of the set with the formula discussed in class. \f0\lang1033\par
{\pntext\f2\'B7\tab}\f1\lang1045 information_gain(before_split, yes, no) - calculates information gain over a split. Gets the probabilities of each split. Then returns: the entropy of the before_split minus the sum of probability of yes times the entropy of yes subset and probability of no times the entropy of no subset.\f0\lang1033\par
{\pntext\f2\'B7\tab}\f1\lang1045 create_split(dataset, target_feature_index) - splits the data based on target_feature_index. For each sample in the dataset it checks if its value is 0 or 1 and assign it to the proper subset. Returns yes_split and no_split as two numpy arrays. \f0\lang1033\par
{\pntext\f2\'B7\tab}\f1\lang1045 get_best_split(X, Y)- iterates through each feature and finds best split for the data. Concatenates the sample features with the sample label. For each feature it creates a split using create_split function. If the length of one side is 0 then the split is skipped. Then it evaluates the information gain for the split. If the information gain is better than the current best information gain it updates all the variables that are returned. Returns the index of the best feature to split on, the information gain value of that split, the subset of yes samples, and the subset of no samples. \f0\lang1033\par
{\pntext\f2\'B7\tab}\f1\lang1045 build_tree(X,Y,counter)-recursively builds a tree based on training data. There are two possibilities that use the same logic for building a tree except they have different base conditions. The logic behind this function is deciding whether the node should be a leaf or a decision node. The conditions for a decision node are: (1) there are still at least 2 samples to be split, and (2) the information gain is not 0 (not all labels in a subset are the same). An extra condition is max_depth condition. If it's set to -1 then the tree will use only the two mentioned conditions to build a tree. If it's greater than zero then the termination will happen when the tree already have the max_depth value amount of decision nodes. The tree is built as follows: if the conditions are met it means that the node should be an instance of Node class. Therefore the best split is found. If the information gain is greater than 0 call build_tree on the false side and call build_tree on the true side. Return the decision node with the index of the feature that the split was made on, left branch, and, right branch. Else, return a Leaf with the most common value and calculated accuracy. If the max_depth is a positive integer the counter that is passed as a parameter to build tree keeps track of how many decision node levels are already in a tree. It is incremented after each creation.\f0\lang1033\par
{\pntext\f2\'B7\tab}\f1\lang1045 train(X,Y) - calls build tree to initialize the root. Passes a counter as 1 (by creation of the tree we assume it already has 1 decision node).\f0\lang1033\par
{\pntext\f2\'B7\tab}\f1\lang1045 print_tree(node) - prints the tree in preorder traversal manner. For debugging purposes.\f0\lang1033\par
{\pntext\f2\'B7\tab}\f1\lang1045 functions that calculate accuracy - were used for debugging purposes. Now the accuracy is stored in each leaf so they are not used within the code.\f0\lang1033\par
{\pntext\f2\'B7\tab}\f1\lang1045 test_set(X,Y) - returns the accuracy of the test set. Runs through all the samples in the test set and collects the predicted labels returned by the predict_single function. Then it calculates the number of correct predictions and returns the accuracy as the ratio of the number of correct predictions and the number of all predictions.\f0\lang1033\par
{\pntext\f2\'B7\tab}\f1\lang1045 predict_single(x, node)- returns the predicted label of the sample. Recursively runs the sample through a tree to find the classification value. First, it checks if the node is a leaf. If it is it returns the value of the leaf. Then, it checks if the node is a decision node. If it is, it takes the feature index stored in the decision node and checks the value of the sample at that index. If it's zero it makes a recursive call on the false side, else it makes a recursive call on the true side.\f0\lang1033\par

\pard\f1\lang1045 The implementation of the functions specified in the project 1 prompt are hidden within the implementation of the DecisionTree class. In this case, the general functions make the calls to the decision tree class methods to provide the desired output.\par

\pard{\pntext\f2\'B7\tab}{\*\pn\pnlvlblt\pnf2\pnindent0{\pntxtb\'B7}}\fi-360\li720 DT_train_binary(X,Y,max_depth) - This function gets the dimentionality of the samples in the dataset, creates a DecisionTree object and initializes its max_depth and the number of features variables. Then, it calls the DecisionTree train() function. Returns the DecisionTree object.\f0\lang1033\par
{\pntext\f2\'B7\tab}\f1\lang1045 DT_test_binary(X,Y,DT)-returns accuracy of the test set classification. Makes a call to DecisionTree test_set() function.\f0\lang1033\par
{\pntext\f2\'B7\tab}\f1\lang1045 DT_make_prediction(x,DT)-returns a prediction label of the sample x. Checks if the dimensionality of the sample passed into a function is complient with the dimentionality of the samples used in training the decision tree. If yes, calls the DecisionTree predict_single function. Else, throws a message of incompatible dimensionality.\f0\lang1033\par

\pard\par
2. K Nearest Neighbors\par
\par
My KNN algorithm works as follows. For each of the test sample (X_train), Euclidean distance to each of the training samples\f1\lang1045  is calculated\f0\lang1033  and store\f1\lang1045 d\f0\lang1033  in the list. Then \f1\lang1045 the list is \f0\lang1033 zip\f1\lang1045 ped \f0\lang1033 with the training labels and \f1\lang1045 sorted\f0\lang1033  \f1\lang1045 in \f0\lang1033 ascending \f1\lang1045 distance \f0\lang1033 order. Then \f1\lang1045 the algorithm\f0\lang1033  runs through that list and sums K first train labels. Then it checks if the sum is greater or less than 0. If the sum is strictly greater than 0 then the positive label is assigned to the test sample. If the sum is equal or less than 0 then negative label is assigned to the test sample. All labels assigned to test samples are stored in the list. Once the procedure is performed for each sample in test set the accuracy is calculated. It is done by comparing the predicted labels to labels provided in the test set. \f1\lang1045 T\f0\lang1033 he number of correctly predicted labels \f1\lang1045 is calculated \f0\lang1033 and divide\f1\lang1045 d \f0\lang1033 by the number of samples in the test set\f1\lang1045 . The result \f0\lang1033 is returned as an accuracy of the KNN algorithm.\par
\par
3. K-Means Clustering\par
\par
My K-Means Clustering works as follows. First\f1\lang1045 ly,\f0\lang1033  all the dim-1 axes\f1\lang1045  that might have been created when loading the data\f0\lang1033  are removed from the dataset. The sample dimension is calculated, current_mu and last_mu initialized as None. Then the means are initialized. The code handles an invalid entry of the means. If the means have wrong dimensions or are missing\f1\lang1045 ,\f0\lang1033  the program calculates the max and min value of all dimensions and pick\f1\lang1045 s\f0\lang1033  a random mean that is in between these t\f1\lang1045 w\f0\lang1033 o values. This is supposed to narrow down the possible int\f1\lang1045 eger values that\f0\lang1033  the program could pick randomly. Then the program enters a loop. It will go until current\f1\lang1045 _\f0\lang1033 mu is not equal to last\f1\lang1045 _\f0\lang1033 mu. For each loop the clusters are stored in a dictionary of lists. Then inside that loop, for each point of the data set, the distance from each cluster mean is calculated and stored in the list. Then the minimum distance is used to determine which cluster should that sample be assigned to. After all points in the training set are processed the new_mean for each cluster is calculated. Then \f1\lang1045 the means update is performed: \f0\lang1033 last_mu is assigned to current_mu and current_mu is assigned to the new_mean. The loop terminates when \f1\lang1045 last_mu == current_mu\f0\lang1033 .\par
}
 